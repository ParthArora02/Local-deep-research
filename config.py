"""
Local Deep Research Configuration Guide

This configuration file controls the behavior of the research system.

MAIN SETTINGS:
- search_tool (str): Choose which search engine to use
  - "auto": Intelligent selection based on query content (recommended)
  - "local_all": Search only local document collections
  - "wikipedia": General knowledge and facts
  - "arxiv": Scientific papers and research
  - "duckduckgo": General web search (no API key needed)
  - "serp": Google search via SerpAPI (requires API key)
  - "guardian": News articles (requires API key)
  - Any collection name from your local_collections.py

- DEFAULT_MODEL (str): LLM to use for analysis
  - "mistral": Default local model via Ollama
  - "deepseek-r1:14b": Alternative larger model
  - "claude-3-5-sonnet-latest": Claude model (requires API key)
  - "gpt-4o": OpenAI model (requires API key)
  - "mpt-7b": MPT model via VLLM

RESEARCH SETTINGS:
- SEARCH_ITERATIONS (int): Number of research cycles
- QUESTIONS_PER_ITERATION (int): Questions per cycle
- SEARCHES_PER_SECTION (int): Searches per report section
- MAX_SEARCH_RESULTS (int): Results per search query
- MAX_FILTERED_RESULTS (int): Results after relevance filtering

For full documentation, see the README.md
"""

from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_community.llms import VLLM  # Added VLLM import
from dotenv import load_dotenv
from web_search_engines.search_engine_factory import get_search as factory_get_search
from utilties.enums import KnowledgeAccumulationApproach

import os
# Load environment variables
load_dotenv()
# Choose search tool: "serp" or "duckduckgo" (serp requires API key)
search_tool = "auto" # Change this variable to switch between search tools; for only local search "local-all"

KNOWLEDGE_ACCUMULATION = KnowledgeAccumulationApproach.QUESTION # None doesnt work with detailed report. It basically means the questions are seperate on the topic.
# LLM Configuration
OPENAIENDPOINT=False # True + URL + Model Name
OPENROUTER_BASE_URL= "https://openrouter.ai/api/v1"

DEFAULT_MODEL = "mistral"  # try to use the largest model that fits into your GPU memory
DEFAULT_TEMPERATURE = 0.7
MAX_TOKENS = 15000

# VLLM Configuration
VLLM_MAX_NEW_TOKENS = 128
VLLM_TOP_K = 10
VLLM_TOP_P = 0.95
VLLM_TEMPERATURE = 0.8

# Search System Settings
SEARCH_ITERATIONS = 3
QUESTIONS_PER_ITERATION = 3

# Report settings
SEARCHES_PER_SECTION = 3
CONTEXT_CUT_OFF = 10000

# citation handler
ENABLE_FACT_CHECKING = False  # comes with pros and cons. Maybe works better with larger LLMs?

# URL Quality Check (applies to both DDG and SerpAPI)
QUALITY_CHECK_DDG_URLS = True  # Keep True for better quality results.

SEARCH_SNIPPETS_ONLY = False
SKIP_RELEVANCE_FILTER = False 

# Search Configuration (applies to both DDG and SerpAPI)
MAX_SEARCH_RESULTS = 50  
MAX_FILTERED_RESULTS = 5
SEARCH_REGION = "us"
TIME_PERIOD = "y"
SAFE_SEARCH = True
SEARCH_LANGUAGE = "English"

# Output Configuration
OUTPUT_DIR = "research_outputs"


def get_llm(model_name=DEFAULT_MODEL, temperature=DEFAULT_TEMPERATURE):
    """Get LLM instance - easy to switch between models"""
    common_params = {
        "temperature": temperature,
        "max_tokens": MAX_TOKENS,
    }

    if "claude" in model_name:
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY not found in environment variables")
        return ChatAnthropic(
            model=model_name, anthropic_api_key=api_key, **common_params
        )
    elif OPENAIENDPOINT:
        api_key = os.getenv("OPENAI_ENDPOINT_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables")
        return ChatOpenAI(model=model_name, api_key=api_key,openai_api_base=OPENROUTER_BASE_URL, **common_params)
       
    elif "gpt" in model_name:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables")
        return ChatOpenAI(model=model_name, api_key=api_key, **common_params)
        
    elif model_name == "mpt-7b":
        # VLLM configuration for the MPT model
        return VLLM(
            model="mosaicml/mpt-7b",
            trust_remote_code=True,  # mandatory for hf models
            max_new_tokens=VLLM_MAX_NEW_TOKENS,
            top_k=VLLM_TOP_K,
            top_p=VLLM_TOP_P,
            temperature=VLLM_TEMPERATURE,
        )

    else:
        return ChatOllama(model=model_name, base_url="http://localhost:11434", **common_params)


def get_search():
    """Get search tool instance based on config settings"""
    print(f"Creating search engine with tool: {search_tool}")
    engine = factory_get_search(
        search_tool=search_tool,
        llm_instance=get_llm(),
        max_results=MAX_SEARCH_RESULTS,
        region=SEARCH_REGION,
        time_period=TIME_PERIOD,
        safe_search=SAFE_SEARCH,
        search_snippets_only=SEARCH_SNIPPETS_ONLY,
        search_language=SEARCH_LANGUAGE,
        max_filtered_results=MAX_FILTERED_RESULTS 
    )
    
    if engine is None:
        print(f"Failed to create search engine with tool: {search_tool}")
    
    return engine
